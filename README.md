<div align="center">
  <img src="icon.jpg" alt="EuraGovExam icon" style="width: 70%;" />
</div>

<p align="center">
  <b>A Multilingual Multimodal Benchmark from Realâ€‘World Civil Service Exams</b>
  <br>
</p>

<h4 align="center">
  <a href="https://arxiv.org/abs/XXXX.XXXXX" target="_blank">Paper</a> |
  <a href="https://euragovexam.github.io/EuraGovExam/index.html" target="_blank">Leaderboard</a> |
  <a href="https://huggingface.co/datasets/EuraGovExam/EuraGovExam" target="_blank">Dataset</a>
</h4>

---

## Table of Contents

* [Overview](#overview)
* [Dataset](#dataset)
* [Quick Start](#quick-start)
* [Benchmark Tasks](#benchmark-tasks)
* [Baselines & Leaderboard](#baselines--leaderboard)
* [Citation](#citation)
* [License](#license)
* [Contributing](#contributing)
* [Contact](#contact)

## Overview

EuraGovExam is a highâ€‘fidelity **multilingual** & **multimodal** benchmark built from authentic civilâ€‘service examinations spanning **five Eurasian regions** and **17 academic / bureaucratic domains**. Each problem is provided as a single highâ€‘resolution **image** plus four answer choices, requiring models to reason over layout, tables, diagrams and codeâ€‘switched text without additional prompts.

* **Scale:**Â â‰ˆ8â€¯k scanned MCQ items
* **Languages:**Â Korean, Japanese, Traditional Chinese, English, Hindi
* **Domains:**Â Mathematics, Law, Administration, Biology, Chemistry, ... (full list below)
* **Regions:**Â SouthÂ Korea, Japan, Taiwan, India, EuropeanÂ Union

EuraGovExam fills the gap between synthetic academic QA sets and the messy reality of government documents, supporting research on visionâ€‘language models, layout understanding, crossâ€‘lingual reasoning, and instruction following.

## Dataset

### Directory Structure

```
EuraGovExam/
â”œâ”€â”€ images/                # 8â€¯k+ question images (.png)
â”œâ”€â”€ metadata.jsonl         # perâ€‘item fields (id, domain, region, correct_answer, ocr_text, etc.)
â””â”€â”€ README.md
```

### Domains

| #  | Domain         | Share |
| -- | -------------- | ----- |
| 1  | Mathematics    | 11â€¯%  |
| 2  | Administration | 11â€¯%  |
| 3  | Biology        | 11â€¯%  |
| â€¦  | â€¦              | â€¦     |
| 17 | EarthÂ Science  | 4â€¯%   |

### Download via ğŸ¤—Â datasets

```python
from datasets import load_dataset
ds = load_dataset("EuraGovExam/EuraGovExam", split="test")  # 'train' & 'dev' coming soon
print(ds[0])
```

Each row contains:

| field     | type       | description                           |
| --------- | ---------- | ------------------------------------- |
| `image`   | PIL.Image  | Question sheet (RGB, \~600Ã—400â€¯px)    |
| `choices` | List\[str] | Four multipleâ€‘choice options          |
| `answer`  | str        | Groundâ€‘truth letter `"A"`â€“`"D"`       |
| `domain`  | str        | Academic / bureaucratic topic         |
| `region`  | str        | Source country                        |
| `ocr`     | str        | (Optional) OCR string for convenience |

## Quick Start

Evaluate any VLM with a *singleâ€‘image, singleâ€‘choice* interface:

```python
import torch
from PIL import Image
from datasets import load_dataset
from my_vlm import generate_answer  # your model

ds = load_dataset("EuraGovExam/EuraGovExam", split="test")
correct = 0
for item in ds:
    pred = generate_answer(Image.open(item["image"]), item["choices"])
    if pred.upper() == item["answer"]:
        correct += 1
print(f"Accuracy: {correct/len(ds):.2%}")
```

## Benchmark Tasks

EuraGovExam challenges models on:

| Challenge                   | Example                                                         |
| --------------------------- | --------------------------------------------------------------- |
| **Layout parsing**          | Nested tables, multiâ€‘column text                                |
| **Crossâ€‘lingual reasoning** | Korean question referencing English labels                      |
| **Math & numeracy**         | Compute values from visual equations                            |
| **Instruction following**   | Required format *â€œThe answer is X.â€* embedded only in the image |

A detailed task taxonomy is in the [paper](https://arxiv.org/abs/XXXX.XXXXX).

## Baselines & Leaderboard

See the **public leaderboard** for upâ€‘toâ€‘date results. Key snapshot (JulyÂ 2025):

| Model              | Params | Accuracy   |
| ------------------ | ------ | ---------- |
| GPTâ€‘4.1            | â€”      | **54.7â€¯%** |
| GPTâ€‘4o             | â€”      | 42.0â€¯%     |
| Claudeâ€‘Sonnetâ€‘4    | â€”      | 46.8â€¯%     |
| Llamaâ€‘3â€‘Visionâ€‘11B | 11â€¯B   | 10.6â€¯%     |
| Gemmaâ€‘3â€‘32Bâ€‘it     | 32â€¯B   | 19.4â€¯%     |

More details in [`docs/results.md`](docs/results.md).

## Citation

If you use EuraGovExam, please cite:

```bibtex
@inproceedings{euragovexam2026,
  title     = {EuraGovExam: A Multilingual Multimodal Benchmark from Realâ€‘World Civil Service Exams},
  author    = {First Author and Second Author and others},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2026}
}
```

## License

EuraGovExam is released under the **CC BYâ€‘NC 4.0** license. See [`LICENSE`](LICENSE) for details.

## Contributing

Pull requests are welcome! Please read [`CONTRIBUTING.md`](CONTRIBUTING.md) first and be mindful of regional examination copyright.

## Contact

Questions? Create an issue or reach out to **kjsqp1010@semyung.ac.kr**.

