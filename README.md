<div align="center">
  <img src="icon12.jpg" alt="EuraGovExam icon" style="width: 70%;" />
</div>

<p align="center">
  <b>A Multilingual Multimodal Benchmark from Realâ€‘World Civil Service Exams</b>
  <br>
</p>

<h4 align="center">
  <a href="" target="_blank">Paper (Comming Soon!)</a> |
  <a href="https://euragovexam.github.io/EuraGovExam/index.html" target="_blank">Leaderboard</a> |
  <a href="https://huggingface.co/datasets/EuraGovExam/EuraGovExam" target="_blank">Dataset</a>
</h4>

---

## Table of Contents

* [Overview](#overview)
* [Dataset](#dataset)
* [Quick Start](#quick-start)
* [Benchmark Tasks](#benchmark-tasks)
* [Baselines & Leaderboard](#baselines--leaderboard)
* [Citation](#citation)
* [License](#license)
* [Contributing](#contributing)
* [Contact](#contact)

## Overview

EuraGovExam is a highâ€‘fidelity **multilingual** & **multimodal** benchmark built from authentic civilâ€‘service examinations spanning **five Eurasian regions** and **17 academic / bureaucratic domains**. Each problem is provided as a single highâ€‘resolution **image** plus four answer choices, requiring models to reason over layout, tables, diagrams and codeâ€‘switched text without additional prompts.

* **Scale:**Â â‰ˆ8â€¯k scanned MCQ items
* **Languages:**Â Korean, Japanese, Traditional Chinese, English, Hindi
* **Domains:**Â Mathematics, Law, Administration, Biology, Chemistry, ... (full list below)
* **Regions:**Â SouthÂ Korea, Japan, Taiwan, India, EuropeanÂ Union

EuraGovExam fills the gap between synthetic academic QA sets and the messy reality of government documents, supporting research on visionâ€‘language models, layout understanding, crossâ€‘lingual reasoning, and instruction following.

## Dataset



### Domains

| #  | Domain           | Share |
| -- | ---------------- | ----- |
| 1  | Mathematics      | 13.0% |
| 2  | Administration   | 11.6% |
| 3  | Biology          | 11.3% |
| 4  | Law              | 10.2% |
| 5  | Language         | 9.3%  |
| 6  | Engineering      | 8.6%  |
| 7  | Physics          | 6.8%  |
| 8  | Economics        | 5.8%  |
| 9  | Computer Science | 5.5%  |
| 10 | History          | 3.2%  |
| 11 | Medicine         | 3.0%  |
| 12 | Politics         | 2.6%  |
| 13 | Chemistry        | 2.6%  |
| 14 | Geography        | 2.3%  |
| 15 | Philosophy       | 1.6%  |
| 16 | Psychology       | 1.5%  |
| 17 | Earth Science    | 1.5%  |

### Download via ðŸ¤—Â datasets

```python
from datasets import load_dataset
ds = load_dataset("EuraGovExam/EuraGovExam")
```



## Baselines & Leaderboard

See the **public leaderboard** for upâ€‘toâ€‘date results. Key snapshot (JulyÂ 2025):

| Model              | Params | Accuracy   |
| ------------------ | ------ | ---------- |
| GPTâ€‘4.1            | â€”      | **54.7â€¯%** |
| GPTâ€‘4o             | â€”      | 42.0â€¯%     |
| Claudeâ€‘Sonnetâ€‘4    | â€”      | 46.8â€¯%     |
| Llamaâ€‘3â€‘Visionâ€‘11B | 11â€¯B   | 10.6â€¯%     |
| Gemmaâ€‘3â€‘32Bâ€‘it     | 32â€¯B   | 19.4â€¯%     |

More details in [`/data/leaderboard.json`](/data/leaderboard.json).

## Citation

If you use EuraGovExam, please cite:

```bibtex
@inproceedings{euragovexam2026,
  title     = {EuraGovExam: A Multilingual Multimodal Benchmark from Realâ€‘World Civil Service Exams},
  author    = {First Author and Second Author and others},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2026}
}
```

## License

EuraGovExam is released under the **CC BYâ€‘NC 4.0** license. See [`LICENSE`](LICENSE) for details.

## Contributing

Pull requests are welcome! Please read [`CONTRIBUTING.md`](CONTRIBUTING.md) first and be mindful of regional examination copyright.

## Contact

**Comming Soon!**

